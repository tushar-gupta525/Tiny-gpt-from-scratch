{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AKYXuGyLV_5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "249453cf-3006-4b9d-fb62-b5560cf9a492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 1: Setup & Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, os, time\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell 2: Download Tiny Shakespeare text dataset\n",
        "\n",
        "# This will download input.txt (tiny-shakespeare) from Karpathy's repo\n",
        "if not os.path.exists(\"input.txt\"):\n",
        "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt\n",
        "\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Dataset length (characters):\", len(text))\n",
        "print(\"Sample:\\n\", text[:500])\n"
      ],
      "metadata": {
        "id": "5i-vaiZrW2ZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d43af0-4b9b-4cca-fa60-89d0571110b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-09 08:52:31--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2025-12-09 08:52:32 (165 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "Dataset length (characters): 1115394\n",
            "Sample:\n",
            " First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell 3: Build vocabulary, encoder/decoder, and split data\n",
        "\n",
        "# Get all unique characters\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "print(\"Chars:\", ''.join(chars))\n",
        "\n",
        "# Mapping from char to int and back\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "def encode(s: str):\n",
        "    \"\"\"String -> list of ints (token ids)\"\"\"\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "def decode(l):\n",
        "    \"\"\"List of ints -> string\"\"\"\n",
        "    return ''.join([itos[i] for i in l])\n",
        "\n",
        "# Convert entire dataset to a tensor of ints\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(\"Data shape:\", data.shape)\n",
        "\n",
        "# Train/Validation split\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(\"Train size:\", train_data.shape, \"Val size:\", val_data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BivaJL-JymaZ",
        "outputId": "1f5ac0c2-4021-4360-965b-dcb3d5689b63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 65\n",
            "Chars: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Data shape: torch.Size([1115394])\n",
            "Train size: torch.Size([1003854]) Val size: torch.Size([111540])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell 4: Define hyperparameters and get_batch() helper\n",
        "\n",
        "batch_size   = 64    # how many sequences per batch\n",
        "block_size   = 128   # max context length for the model\n",
        "max_iters    = 2000  # you can increase this (e.g. 5000–10000) for better quality\n",
        "eval_interval = 200\n",
        "learning_rate = 3e-4\n",
        "eval_iters   = 200  # how many batches to average for loss estimate\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch(split):\n",
        "    \"\"\"Generate a batch of inputs (x) and targets (y) for training.\"\"\"\n",
        "    data_split = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
        "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# Quick sanity check\n",
        "xb, yb = get_batch('train')\n",
        "print(\"Batch x shape:\", xb.shape)  # (B, T)\n",
        "print(\"Batch y shape:\", yb.shape)  # (B, T)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghAGHmmdym21",
        "outputId": "6346fee5-fe4f-48e2-fae5-80f233b7b91c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch x shape: torch.Size([64, 128])\n",
            "Batch y shape: torch.Size([64, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell 5: Define the mini-GPT transformer model\n",
        "\n",
        "n_embed = 384   # embedding dimension\n",
        "n_head  = 6     # number of attention heads\n",
        "n_layer = 6     # number of transformer blocks\n",
        "dropout = 0.2\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"One head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B, T, head_size)\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "\n",
        "        # compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) / math.sqrt(k.size(-1))  # (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # weighted sum of values\n",
        "        v = self.value(x)  # (B, T, head_size)\n",
        "        out = wei @ v      # (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj  = nn.Linear(num_heads * head_size, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Simple MLP with one hidden layer\"\"\"\n",
        "\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication (attention) then computation (MLP)\"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # token and position embeddings\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        # transformer blocks\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)  # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # token + positional embeddings\n",
        "        tok_emb = self.token_embedding_table(idx)                   # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T, C)\n",
        "        x = tok_emb + pos_emb                                       # (B, T, C)\n",
        "\n",
        "        # transformer blocks\n",
        "        x = self.blocks(x)                                          # (B, T, C)\n",
        "        x = self.ln_f(x)                                            # (B, T, C)\n",
        "        logits = self.lm_head(x)                                    # (B, T, vocab_size)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\"Autoregressively generate new tokens from the model.\"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]  # crop to last block_size tokens\n",
        "            logits, _ = self(idx_cond)       # (B, T, vocab_size)\n",
        "            logits = logits[:, -1, :]        # take the last time step\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, vocab_size)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # append\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel().to(device)\n",
        "print(\"Model parameters:\", sum(p.numel() for p in model.parameters()) / 1e6, \"M\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS2uXhU6ym5K",
        "outputId": "f8fa1e5f-5d5c-469d-ac0b-e206fac3fbbf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 10.739777 M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell 6: Loss evaluation helper (train & val)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = []\n",
        "        for _ in range(eval_iters):\n",
        "            xb, yb = get_batch(split)\n",
        "            _, loss = model(xb, yb)\n",
        "            losses.append(loss.item())\n",
        "        out[split] = sum(losses) / len(losses)\n",
        "    model.train()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "cGvi836Hym7Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell 7: Train the model\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # Occasional evaluation\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"Step {iter:4d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}  (elapsed {elapsed:.1f}s)\")\n",
        "\n",
        "    # Get batch\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Forward\n",
        "    logits, loss = model(xb, yb)\n",
        "    # Backward\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Training finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkwt3kgFym9p",
        "outputId": "ff6110e3-3498-4554-88af-0d573d29e426"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step    0: train loss 4.3336, val loss 4.3296  (elapsed 27.9s)\n",
            "Step  200: train loss 2.2694, val loss 2.2926  (elapsed 103.7s)\n",
            "Step  400: train loss 1.8955, val loss 1.9856  (elapsed 179.0s)\n",
            "Step  600: train loss 1.7057, val loss 1.8554  (elapsed 254.6s)\n",
            "Step  800: train loss 1.5995, val loss 1.7765  (elapsed 329.8s)\n",
            "Step 1000: train loss 1.5203, val loss 1.7114  (elapsed 405.2s)\n",
            "Step 1200: train loss 1.4683, val loss 1.6730  (elapsed 480.4s)\n",
            "Step 1400: train loss 1.4346, val loss 1.6431  (elapsed 555.4s)\n",
            "Step 1600: train loss 1.3986, val loss 1.6134  (elapsed 630.3s)\n",
            "Step 1800: train loss 1.3679, val loss 1.5884  (elapsed 705.1s)\n",
            "Step 1999: train loss 1.3468, val loss 1.5765  (elapsed 779.9s)\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell 8: Generate text from an empty prompt\n",
        "\n",
        "model.eval()\n",
        "start_context = torch.zeros((1, 1), dtype=torch.long, device=device)  # start token = 0\n",
        "with torch.no_grad():\n",
        "    generated = model.generate(start_context, max_new_tokens=500)\n",
        "\n",
        "print(decode(generated[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBGbFomQynAD",
        "outputId": "88b18d32-65fd-470c-e5ee-f6c45c5c9a2f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mether:\n",
            "Mine man, that woll I envious sing them;\n",
            "Forgelow's absence, for thou didst answer that,\n",
            "Be know a kneat cain the bobe bare hateful life;\n",
            "Lest him hath dine to here will his poor so--\n",
            "And that thou hast are that thou comes be a courten?\n",
            "\n",
            "First Citizen:\n",
            "Why, sworn, I would we about not from to this pirce should\n",
            "witch heir dues and revenge to it: he'll, we in\n",
            "Be brook to the noble find, and purch's heart with says\n",
            "Alandly deads, as it such from the viyors\n",
            "Maketage it aland even death. This\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell 9: Prompt-based generation + interactive loop\n",
        "\n",
        "model.eval()\n",
        "\n",
        "def generate_from_prompt(prompt: str, max_new_tokens: int = 200) -> str:\n",
        "    \"\"\"\n",
        "    Take a text prompt (string), run it through the model,\n",
        "    and return the generated continuation (string).\n",
        "    \"\"\"\n",
        "    if len(prompt.strip()) == 0:\n",
        "        # behave like empty context\n",
        "        idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    else:\n",
        "        encoded = encode(prompt)\n",
        "        idx = torch.tensor([encoded], dtype=torch.long, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    return decode(out[0].tolist())\n",
        "\n",
        "\n",
        "# 1) Quick test with a fixed prompt\n",
        "test_prompt = \"ROMEO:\"\n",
        "print(\"=== Test prompt ===\")\n",
        "print(\"Prompt:\", repr(test_prompt))\n",
        "print(\"Output:\\n\")\n",
        "print(generate_from_prompt(test_prompt, max_new_tokens=300))\n",
        "\n",
        "\n",
        "# 2) Optional: interactive loop (works in Colab terminal/console cell)\n",
        "print(\"\\n--- Interactive mode ---\")\n",
        "print(\"Type a prompt and press Enter. Type 'quit' to stop.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_inp = input(\"Your prompt: \")\n",
        "    if user_inp.lower() in {\"quit\", \"exit\", \"q\"}:\n",
        "        print(\"Bye!\")\n",
        "        break\n",
        "\n",
        "    out_text = generate_from_prompt(user_inp, max_new_tokens=250)\n",
        "    print(\"\\nModel output:\\n\")\n",
        "    print(out_text)\n",
        "    print(\"\\n\" + \"-\" * 80 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et9EssktynYa",
        "outputId": "b0164c73-d6b0-4806-defb-346c326f0b9a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Test prompt ===\n",
            "Prompt: 'ROMEO:'\n",
            "Output:\n",
            "\n",
            "ROMEO:\n",
            "I am, no, who that friar, I speak with the plimment\n",
            "In thy goodness, this flance, time arms carqued\n",
            "From hindered, and Marcius and wretched by night.\n",
            "\n",
            "DUKE OF YORK:\n",
            "Ay, shall how so us.\n",
            "\n",
            "LUCIO:\n",
            "I turn, sir, and you\n",
            "Is as the feel's cocking and Here's hand:\n",
            "O, my wrong is Camillo made to a privillow\n",
            "\n",
            "--- Interactive mode ---\n",
            "Type a prompt and press Enter. Type 'quit' to stop.\n",
            "\n",
            "Your prompt: that's beautiful evening \n",
            "\n",
            "Model output:\n",
            "\n",
            "that's beautiful evening man;\n",
            "That I may if mock'd my fool with go conquarl.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Nay, have you bids, and I follow us gentrans she\n",
            "Soul, I should best were every you are a snab\n",
            "Through of find you.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "O, gogua O: we do, forget, for my hope.\n",
            "\n",
            "GRERD \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Your prompt: science\n",
            "\n",
            "Model output:\n",
            "\n",
            "science have fellow'd you.\n",
            "The cendicious heart condemned\n",
            "If his drink din his partue proaply;\n",
            "And should me not by foul free from his law,\n",
            "Until petty the stop's words' stands;\n",
            "But go have born'd, and all begence;\n",
            "And take his more consent; are so comes st\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Your prompt: AI is\n",
            "\n",
            "Model output:\n",
            "\n",
            "AI is tall some that i' throw.\n",
            "May it will be store you on we,\n",
            "And fellow may heads ever tonguish\n",
            "It alonion one to the law, promise.\n",
            "Do not him, stay unto the world.\n",
            "But upon you and, again.\n",
            "\n",
            "LUCIO:\n",
            "It I should so.\n",
            "\n",
            "For Clarence:\n",
            "You, it is for love?\n",
            "\n",
            "IS\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Your prompt: this night and day\n",
            "\n",
            "Model output:\n",
            "\n",
            "this night and day's morning.\n",
            "The ends, to mercy moves him, nor he sea.\n",
            "Why, pain, she had not been in here?\n",
            "Sure when as this lose dry shall that seal me?\n",
            "Unvain, what was it in great issue?\n",
            "\n",
            "FLORIZEL:\n",
            "Take, and a throw of mine earch\n",
            "And good false and old night nigh\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Your prompt: quit\n",
            "Bye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l5Y1gLt_3zVn"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}